import copy
import random
from typing import Tuple, Any, Optional

import numpy as np
from secml.adv.attacks import CAttackEvasion
from secml.array import CArray
from secml.data import CDataset

from secml_malware.attack.blackbox.c_blackbox_problem import CBlackBoxProblem

try:
    import nevergrad
    from nevergrad.optimization.differentialevolution import DifferentialEvolution
except ImportError:
    raise ImportError(
        "Install NEVERGRAD to apply black box evasion")


def _pad_sequence_with_last(sequence, until):
    how_many = until - len(sequence)
    if how_many <= 0:
        return sequence
    return sequence + [sequence[-1]] * how_many


class CNevergradGeneticAlgorithm(CAttackEvasion):
    """
    The genetic optimizer.
    """

    def _objective_function_gradient(self, x):
        raise NotImplementedError("This function is not used inside secML malware")

    def _objective_function(self, x):
        raise NotImplementedError("This function is not used inside secML malware")

    def objective_function_gradient(self, x):
        raise NotImplementedError("This function is not used inside secML malware")

    def objective_function(self, x):
        raise NotImplementedError("This function is not used inside secML malware")

    def f_eval(self):
        raise NotImplementedError("This function is not used inside secML malware")

    def grad_eval(self):
        raise NotImplementedError("This function is not used inside secML malware")

    def __init__(
            self,
            problem: CBlackBoxProblem,
            is_debug: bool = False,
            random_state: Optional[int] = None,
    ):
        """
        Create and instance of the genetic optimizer.

        Parameters
        ----------
        problem : CBlackBoxProblem
            The problem to optimize
        is_debug : bool, default False
            If True, debug prints will be displayed during the attack.
            Default is False
        """
        CAttackEvasion.__init__(
            self,
            problem.model_wrapper.classifier,
            problem.model_wrapper.classifier,
        )
        self.problem = problem
        self.confidences_ = []
        self.fitness_ = []
        self.sizes_ = []
        self.changes_per_iterations_ = []
        self.model_wrapper = problem.model_wrapper
        self.is_debug = is_debug
        self._original_x = None
        self.minimization_result_ = []
        self.evolved_problem_ = None
        self.stagnation = 5
        self.elapsed_time_ = 0
        self.random_state = random_state

    def run(self, x, y, ds_init=None) -> Tuple[CArray, CArray, CDataset, Any]:
        """
        Runs the genetic algorithms.

        Parameters
        ----------
        x : CArray
            input sample to perturb
        y : CArray
            original class
        ds_init : CDataset, optional, default None
            the initialization point.
            Default is None
        Returns
        -------
        CArray
            y_pred : the predicted label after the attack
        CArray
            scores : the scores after the attack
        CDataset
            adv_ds : the CDataset containing the adversarial points
        CArray
            f_obj : the mean value for the objective function
        """
        x = CArray(x).atleast_2d()
        y = CArray(y).atleast_2d()
        x_init = None if ds_init is None else CArray(ds_init.X).atleast_2d()

        # only consider samples that can be manipulated
        v = self.is_attack_class(y)
        idx = CArray(v.find(v)).ravel()
        # print(v, idx)

        # number of modifiable samples
        n_mod_samples = idx.size

        adv_ds = CDataset(x.deepcopy(), y.deepcopy())

        # If dataset is sparse, set the proper attribute
        if x.issparse is True:
            self._issparse = True

        # array in which the value of the optimization function are stored
        fs_opt = CArray.zeros(n_mod_samples, )
        y_pred = CArray.zeros(n_mod_samples, )
        scores = CArray.zeros((n_mod_samples, 2))
        for i in range(n_mod_samples):
            k = idx[i].item()  # idx of sample that can be modified

            xi = x[k, :] if x_init is None else x_init[k, :]
            x_opt, f_opt = self._run(x[k, :], y[k], x_init=xi)

            self.logger.info(
                "Point: {:}/{:}, f(x):{:}, eval:{:}/{:}".format(k, x.shape[0], f_opt, self.f_eval, self.grad_eval))
            if x_opt.shape[-1] > adv_ds.X.shape[-1]:
                # Need to resize the whole adv dataset, since CDataset can't deal with varying vector sizes
                new_length = x_opt.shape[-1]
                adv_ds.X = adv_ds.X.resize((adv_ds.X.shape[0], new_length), 256)
            adv_ds.X[k, :min(adv_ds.X.shape[-1], x_opt.shape[-1])] = x_opt
            fs_opt[i] = f_opt
            y_p, score = self.problem.model_wrapper.predict(x_opt, return_decision_function=True)
            scores[i, :] = score[0, :]
            y_pred[i] = y_p

        # Return the mean objective function value on the evasion points (
        # computed from the outputs of the surrogate classifier)
        f_obj = fs_opt.mean()

        return y_pred, scores, adv_ds, f_obj

    def _run(self, x0, y0, x_init=None):

        if x_init is None:
            x_init = copy.deepcopy(x0)

        self._original_x = self.problem.init_starting_point(x_init)
        current_conf = self.problem.score_step(x_init, penalty_term=self.problem.penalty_regularizer)

        if self.is_debug:
            print(f'> Original Confidence: {current_conf}')
            print("> Beginning new sample evasion...")

        minimization_results = self._compute_black_box_optimization()

        self.minimization_result_ = minimization_results.tondarray()

        x_adv = self.problem.apply_feasible_manipulations(self.minimization_result_, self._original_x)
        best_confidence = self.problem.score_step(x_adv, 0)
        if self.is_debug:
            print(f'>AFTER INVERSION, CONFIDENCE SCORE: {best_confidence}')
        return x_adv, best_confidence

    def _compute_black_box_optimization(self) -> CArray:
        ng_optim = DifferentialEvolution(popsize=self.problem.population_size, crossover="twopoints")
        parametrization = nevergrad.p.Array(shape=(1, self.problem.latent_space_size), lower=0.0, upper=1)
        if self.random_state is not None:
            parametrization.random_state = np.random.RandomState(self.random_state)
        ng_optim = ng_optim(parametrization=parametrization,
                            budget=self.problem.iterations * self.problem.population_size)
        for i in range(ng_optim.budget):
            delta_t = ng_optim.ask()
            adv_malware = self.problem.apply_feasible_manipulations(delta_t.value[0], self._original_x)
            loss = self.problem.score_step(adv_malware, self.problem.penalty_regularizer)
            ng_optim.tell(delta_t, loss)
        return CArray(ng_optim.provide_recommendation().value[0])

    @classmethod
    def write_adv_to_file(cls, x_adv: CArray, path: str):
        """
        Write the adversarial malware as a file on disk

        Parameters
        ----------
        x_adv : CArray
            The adversarial malware to dump
        path : str
            The path where to save the executable
        """
        x_real = x_adv.tolist()[0]
        x_real_adv = b''.join([bytes([i]) for i in x_real])
        with open(path, 'wb') as f:
            f.write(x_real_adv)


def random_mutation(individual, indpb):
    """
    Apply the mutation operator, that perturb randomly each entry of the individual, with a given probability.
    The mutation is applied in-place.

    Parameters
    ----------
    individual :
        the individual to mutate
    indpb : float
        the probability of altering a single entry
    Returns
    -------
    tuple
        the mutated individual, the mutatio is in-place
    """
    size = len(individual)
    for i in range(size):
        if random.random() < indpb:
            individual[i] = random.random()
    return individual,
